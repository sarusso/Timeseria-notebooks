{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40711aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae92591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.rendered_html { font-size: 15px; }</style>\"))\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level='INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e3ba4",
   "metadata": {},
   "source": [
    "# Timeseria quickstart\n",
    "\n",
    "Timeseria is a data processing library which aims at making it easy to handle time series data and to build statistical and machine learning models on top of it.\n",
    "\n",
    "It comes with a built-in set of common operations (resampling, slotting, differencing etc.) as well as \n",
    "models (reconstruction, forecasting and anomaly detection), and both custom operations and models can be easily plugged in.\n",
    "\n",
    "Timeseria also tries to address by design all those annoying things which are often left as an implementation detail but that actually cause wasting massive amounts of time - as handling data losses, non-uniform sampling rates, differences between time-slotted data and punctual observations, variable time units, timezones, DST changes and so on.\n",
    "\n",
    "This is a (super) quickstart, if you are looking for a more structured introduction, have a look at the [welcome notebook](https://sarusso.github.io/Timeseria/Welcome.html). Also the [reference documentation](https://timeseria.readthedocs.io) might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245263a9",
   "metadata": {},
   "source": [
    "## Load some data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fdedf",
   "metadata": {},
   "source": [
    "Let's load some data: an indoor temperature winter dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseria import storages\n",
    "DATASET_PATH = '/'.join(storages.__file__.split('/')[0:-1]) + '/tests/test_data/csv/'\n",
    "\n",
    "temperature_timeseries = storages.CSVFileStorage(DATASET_PATH + 'temperature_winter.csv').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c83142",
   "metadata": {},
   "source": [
    "Let's have a look at the time series we just loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357450e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temperature_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15d220",
   "metadata": {},
   "source": [
    "Now plot the data, using Timeseria built-in plotting engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f96972",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_timeseries.plot(aggregate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f3e30",
   "metadata": {},
   "source": [
    "If you zoom in, you will see that data has been aggregated and plotted as a line chart plus an area chart (which reprensents the minimum and maximum values boundaries before aggregating the data). In this way you can plot even millions of data points wihtout slowing down the plot or crashing your browser, without loosing much information about peaks and spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d9196d",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Resample (and make data uniform)\n",
    "\n",
    "Now resample the time series at one hour sampling interval, also making data uniform and equally spaced over time. Gaps are filled by linear interpolation (the default interpolation method for the resampling) and the \"data loss\" index is added as data point attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00241ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_timeseries = temperature_timeseries.resample('1h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522002f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_timeseries[58].data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ef3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_timeseries.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d9002",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Reconstruct missing data\n",
    "\n",
    "Use a simple, periodic average-based model for reconstructing missing data. Fit on about 80% of the data and test on the rest, with 6, 12, and 24 hours gaps. Limit the evaluation to 100 samples per step to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650115ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseria.models import PeriodicAverageReconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paverage_reconstructor = PeriodicAverageReconstructor()\n",
    "paverage_reconstructor.fit(temperature_timeseries[0:2000])\n",
    "paverage_reconstructor.evaluate(temperature_timeseries[2000:], steps=[6,12,24], limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1c156",
   "metadata": {},
   "source": [
    "Apply the reconstruction model. By default only full (100%) data losses are recontructed, as Timeseria considers data points with lower da losses (i.e. because in the interpolation process they were sitting in between existing and missing data points) as still representative. Also, a \"data_reconstructed\" index is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_timeseries = paverage_reconstructor.apply(temperature_timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15226ed8",
   "metadata": {},
   "source": [
    "...and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8904292",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_timeseries.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f381e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_timeseries[0].data_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882771e",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "## Three days hourly temperature forecast\n",
    "\n",
    "Use a LSTM neural network mdoel to to forecast three days of temperatures, but before fitting run a cross validation to get an idea about the accuracy you can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb255906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseria.models import LSTMForecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9307f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LSTM_forecaster = LSTMForecaster(window=12, neurons=64, features=['values', 'diffs', 'hours'])\n",
    "LSTM_forecaster.cross_validate(temperature_timeseries, rounds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531dbcf2",
   "metadata": {},
   "source": [
    "Now fit, apply and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_forecaster.fit(temperature_timeseries)\n",
    "LSTM_forecaster.apply(temperature_timeseries,n=72).plot(indexes=['forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b6ad1",
   "metadata": {},
   "source": [
    "\n",
    "<br />\n",
    "\n",
    "## Run an anomaly detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec2227",
   "metadata": {},
   "source": [
    "Use the periodic average anomaly detection model, which will consider a data point value an anomaly if too far from its periodic average. This will add an \"anomaly\" index as data points attibute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseria.models import PeriodicAverageAnomalyDetector\n",
    "paverage_anomaly_detector = PeriodicAverageAnomalyDetector()\n",
    "paverage_anomaly_detector.fit(temperature_timeseries, stdevs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749a35b",
   "metadata": {},
   "source": [
    "Apply and plot, but show only the \"anomaly\" index to improve plot readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paverage_anomaly_detector.apply(temperature_timeseries).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85e97e",
   "metadata": {},
   "source": [
    "## Altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672a97b",
   "metadata": {},
   "source": [
    "Apply again both models, but this time on the same time series, and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6dabcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anoamaly_temperature_timeseries = paverage_anomaly_detector.apply(temperature_timeseries)\n",
    "forecast_and_anomaly_temperature_timeseries = LSTM_forecaster.apply(anoamaly_temperature_timeseries, n=72)\n",
    "forecast_and_anomaly_temperature_timeseries.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5737c16",
   "metadata": {},
   "source": [
    "## Move to daily data\n",
    "\n",
    "Slot the time series into 1-day slots, also computing the min and max operations besideds the default average one, but change the timezone before to get the right daily aggregates and to properly take into account the DST change. Slotting in days is indeed different than slotting in 24 hours, they are just two different time units: the first is variable, the second is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb50f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forecast_and_anomaly_temperature_timeseries.change_timezone('Europe/Rome')\n",
    "from timeseria.operations import min, max\n",
    "forecast_and_anomaly_temperature_timeseries.slot(unit='1D', extra_operations=[min,max]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acac032",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Next steps\n",
    "\n",
    "You can have a look at the [welcome tutorial](https://sarusso.github.io/Timeseria/Welcome.html), which provides a more structired introduction on Timeseria datastructures and their philosophy, as well as practical examples on both buiilt-in and custom models and operations.\n",
    "\n",
    "You can also have a look at the example repository ([Timeseria-notebooks](https://github.com/sarusso/Timeseria-notebooks)), which is also ready to be play wiht in [Binder](https://mybinder.org/v2/gh/sarusso/Timeseria-notebooks/HEAD), together with this quickstart and the welcome tutorial.\n",
    "\n",
    "<br/>\n",
    "Or, you can give it a try in your own projects:\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/sarusso/Timeseria.git\n",
    "```\n",
    "\n",
    "..or you can contribute! :)\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
